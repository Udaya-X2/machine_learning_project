{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from matplotlib_venn import venn3_unweighted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (confusion_matrix, hinge_loss, log_loss,\n",
    "                             make_scorer, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, learning_curve,\n",
    "                                     train_test_split, validation_curve)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearnex.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Cleaning</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in survey data.\n",
    "df = pd.read_csv('survey_results_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all non-students.\n",
    "students = df[df['Employment'].str.contains('Student, full-time', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with mostly missing values.\n",
    "nalimit = len(students) * 0.4\n",
    "students = students.dropna(axis='columns', thresh=nalimit)\n",
    "\n",
    "# Drop survey-related features.\n",
    "del students['ResponseId']\n",
    "students = students.loc[:, ~students.columns.str.contains(\"SO|Survey\")]\n",
    "\n",
    "# Reset dataframe indices.\n",
    "students = students.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Data Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the domain of each feature in the dataset.\n",
    "def explore_domain(df: pd.DataFrame):\n",
    "    for feature in df:\n",
    "        responses = set()\n",
    "\n",
    "        # Many survey questions have multi-answer responses. Split them\n",
    "        # to count the number of unique responses to each question.\n",
    "        for response in df[feature]:\n",
    "            responses |= response_to_set(response)\n",
    "\n",
    "        # Display the number of possible responses, how many missing\n",
    "        # responses there are, and list each possible response.\n",
    "        print(f'{feature} (Count: {len(responses)}, Null: '\n",
    "              f'{df[feature].isna().sum()}): '\n",
    "              f'{\"; \".join(sorted(responses))}\\n')\n",
    "\n",
    "\n",
    "# Converts a multi-answer survey response into a set of sub-responses.\n",
    "def response_to_set(response: str):\n",
    "    return set(response.split(';')) if pd.notna(response) else set()\n",
    "\n",
    "\n",
    "# Counts the number of sub-responses given to a multi-answer question.\n",
    "def count_responses(response: str):\n",
    "    return response.count(';') + 1 if pd.notna(response) else 0\n",
    "\n",
    "\n",
    "# explore_domain(students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Feature Selection</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data that can either be counted or categorized into features.\n",
    "countable_data = ['LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith',\n",
    "                  'PlatformHaveWorkedWith', 'WebframeHaveWorkedWith',\n",
    "                  'MiscTechHaveWorkedWith', 'ToolsTechHaveWorkedWith']\n",
    "categorical_data = ['MainBranch', 'EdLevel', 'Age']\n",
    "encoding = [\n",
    "    {\n",
    "        'I used to be a developer by profession, but no longer am': 0,\n",
    "        ('I am not primarily a developer, but I write '\n",
    "         'code sometimes as part of my work'): 1,\n",
    "        'I am learning to code': 2,\n",
    "        'I code primarily as a hobby': 3,\n",
    "        'I am a developer by profession': 4\n",
    "    },\n",
    "    {\n",
    "        'Something else': 0,\n",
    "        'Primary/elementary school': 1,\n",
    "        ('Secondary school (e.g. American high school, '\n",
    "         'German Realschule or Gymnasium, etc.)'): 2,\n",
    "        'Some college/university study without earning a degree': 3,\n",
    "        'Associate degree (A.A., A.S., etc.)': 4,\n",
    "        'Professional degree (JD, MD, etc.)': 5,\n",
    "        'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 6,\n",
    "        'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 7,\n",
    "        'Other doctoral degree (Ph.D., Ed.D., etc.)': 8\n",
    "    },\n",
    "    {\n",
    "        'Prefer not to say': 0,\n",
    "        'Under 18 years old': 1,\n",
    "        '18-24 years old': 2,\n",
    "        '25-34 years old': 3,\n",
    "        '35-44 years old': 4,\n",
    "        '45-54 years old': 5,\n",
    "        '55-64 years old': 6,\n",
    "        '65 years or older': 7\n",
    "    }\n",
    "]\n",
    "os_data = ['OpSysPersonal use', 'OpSysProfessional use']\n",
    "\n",
    "# Isolate the IDEs column; this will be used in the output vector.\n",
    "output = 'NEWCollabToolsHaveWorkedWith'\n",
    "\n",
    "# Generate features by counting the number of responses to a question.\n",
    "data = students[countable_data].applymap(count_responses)\n",
    "data.columns = data.columns.map(lambda x: x.removesuffix('HaveWorkedWith'))\n",
    "\n",
    "# Generate features by encoding distinct survey responses as digits.\n",
    "for i, feature in enumerate(categorical_data):\n",
    "    data[feature] = students[feature].map(encoding[i])\n",
    "\n",
    "# Generate boolean features based on employment status, whether the\n",
    "# respondent uses WSL, and use of version control systems (i.e., Git).\n",
    "data['Employed'] = students['Employment'].str.contains('Employed')\n",
    "\n",
    "os = students[os_data].applymap(response_to_set)\n",
    "os = os.apply(lambda x: set.union(*x), axis=1)\n",
    "data['WSL'] = os.map({'Windows Subsystem for Linux (WSL)'}.issubset)\n",
    "\n",
    "data['VCS'] = students['VersionControlSystem'].fillna(\"I don't use one\")\n",
    "data['VCS'] = data['VCS'].map(lambda x: x != \"I don't use one\")\n",
    "\n",
    "# Increase feature based on whether the VCS was used in an editor/IDE.\n",
    "data['VCS'] += students['VCInteraction'].str.contains('Code editor', na=0)\n",
    "\n",
    "# Generate additional numeric features based on the number of operating\n",
    "# systems used by the respondent and how many years they have coded.\n",
    "data['OS'] = os.map(len)\n",
    "data['YearsCode'] = students['YearsCode'].replace('Less than 1 year', 1)\n",
    "data['YearsCode'] = data['YearsCode'].replace('More than 50 years', 50)\n",
    "\n",
    "# Classify respondents by whether they use VS Code.\n",
    "data['VSCode'] = students[output].str.contains('Visual Studio Code', na=0)\n",
    "\n",
    "# Clean missing values and convert boolean features to integers.\n",
    "data = data.fillna(0).astype(int)\n",
    "\n",
    "# Generate the design matrix and output vector.\n",
    "X = data.drop('VSCode', axis=1)\n",
    "y = data['VSCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Feature Analysis</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap showing the pairwise correlation between features.\n",
    "def plot_correlation_matrix(df: pd.DataFrame):\n",
    "    corr = df.corr()\n",
    "    plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True)\n",
    "\n",
    "\n",
    "# Select data for \"want to\"-based multi-answer questions.\n",
    "other_countable_data = ['LanguageWantToWorkWith', 'DatabaseWantToWorkWith',\n",
    "                        'PlatformWantToWorkWith', 'WebframeWantToWorkWith',\n",
    "                        'MiscTechWantToWorkWith', 'ToolsTechWantToWorkWith']\n",
    "\n",
    "# Count the number of responses to questions regarding what the\n",
    "# respondent has worked with and what they want to work with.\n",
    "df_count = students[countable_data + other_countable_data]\n",
    "df_count = df_count.applymap(count_responses)\n",
    "\n",
    "# There is a strong correlation between the number of responses\n",
    "# to \"have worked with\" and \"want to work with\" questions.\n",
    "# plot_correlation_matrix(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a stacked bar graph showing the proportion of VS Code\n",
    "# users to non-VS Code users in the given feature category.\n",
    "def plot_vscode_by_category(df: pd.DataFrame, x: str):\n",
    "    # Use confusion matrix to get VS Code users by category.\n",
    "    cm = confusion_matrix(df[x], df['VSCode'])\n",
    "    non_vscode_users = cm[:, 0]\n",
    "    vscode_users = cm[:, 1]\n",
    "\n",
    "    # Compute proportions of VS Code vs. non-VS Code.\n",
    "    total = non_vscode_users + vscode_users\n",
    "    prop_non_vscode = np.true_divide(non_vscode_users, total) * 100\n",
    "    prop_vscode = np.true_divide(vscode_users, total) * 100\n",
    "\n",
    "    # Get positions of bars on the x-axis.\n",
    "    x_axis = sorted(df[x].unique())\n",
    "    num_cat = len(x_axis)\n",
    "    x_axis = x_axis if num_cat > 2 else ['No', 'Yes']\n",
    "    r = range(num_cat)\n",
    "\n",
    "    # Plot top and bottom bars.\n",
    "    plt.figure(figsize=(5 * num_cat ** 0.25, 5 * num_cat ** 0.25))\n",
    "    barWidth = 0.8\n",
    "    plt.bar(r, prop_non_vscode, bottom=prop_vscode, color='#ff4000',\n",
    "            edgecolor='white', width=barWidth, label=\"Doesn't Use VS Code\")\n",
    "    plt.bar(r, prop_vscode, color='#00bfff', edgecolor='white',\n",
    "            width=barWidth, label=\"Uses VS Code\")\n",
    "\n",
    "    # Plot category ticks, axis labels, and legend.\n",
    "    plt.xticks(r, x_axis)\n",
    "    plt.xlabel(x, fontweight='bold')\n",
    "    plt.ylabel(\"Survey Respondents (%)\", fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# for feature in X:\n",
    "#     plot_vscode_by_category(data, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the importance of each feature, in descending order.\n",
    "def show_feature_importance(X: pd.DataFrame, y: pd.Series):\n",
    "    # Run a tree-based classifier to determine feature importance.\n",
    "    dt = DecisionTreeClassifier(random_state=42, criterion='log_loss')\n",
    "    dt.fit(X, y)\n",
    "\n",
    "    # Compose features/importances in a dataframe and display said data.\n",
    "    fi_df = pd.DataFrame(zip(X.columns, dt.feature_importances_),\n",
    "                         columns=['Feature', 'Importance'])\n",
    "    fi_df.sort_values('Importance', ascending=False, inplace=True)\n",
    "    display(fi_df.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# show_feature_importance(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a pie chart describing the proportion of VS Code users.\n",
    "def plot_vscode_users(y: pd.Series):\n",
    "    colors = colors = ['#56a5d8', '#7b6ca7']\n",
    "    plt.pie(y.value_counts(), colors=colors, autopct='%.2f%%', startangle=90)\n",
    "    plt.title('Students Who Use Visual Studio Code')\n",
    "    plt.legend(ncol=2, labels=['Uses VS Code', \"Doesn't Use VS Code\"],\n",
    "               bbox_to_anchor=(1, 0), loc='best')\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# The dataset is imbalanced. Most respondents use VS Code.\n",
    "# plot_vscode_users(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Applying Machine Learning Algorithms</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (60%), validation (20%), and test (20%) sets.\n",
    "def train_val_test_split(X, y, seed):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.6,\n",
    "                                                      random_state=seed)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_val, y_val,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, 42)\n",
    "\n",
    "# Data used to convert cross-validation to hold-out validation.\n",
    "# Most sklearn functions with a cv parameter rely on a k-fold split.\n",
    "# We replace this with a 2-fold split: the training/validation set.\n",
    "cv = [(X_train.index, X_val.index)]\n",
    "\n",
    "# Print the dimensions of each dataset.\n",
    "# print(f'Training Data: {X_train.shape}')\n",
    "# print(f'Testing Data: {X_test.shape}')\n",
    "# print(f'Validation Data: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the final estimator in a pipeline.\n",
    "def get_model_name(estimator):\n",
    "    return list(estimator.named_steps)[-1]\n",
    "\n",
    "\n",
    "# Get the name of a parameter used in a pipeline.\n",
    "def get_param_name(parameter):\n",
    "    return parameter.partition(\"__\")[-1]\n",
    "\n",
    "\n",
    "# Returns the set of labels this model misclassified during testing.\n",
    "def misclassified_labels(estimator):\n",
    "    return set(np.where(y_test != estimator.predict(X_test))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a validation curve by varying the given hyperparameter.\n",
    "def plot_validation_curve(estimator, param_name, param_values, cost=None):\n",
    "    # Compute the errors of the training and validation data\n",
    "    # across various different hyperparameter values.\n",
    "    train_err, val_err = validation_curve(estimator=estimator, X=X, y=y,\n",
    "                                          param_name=param_name,\n",
    "                                          param_range=param_values,\n",
    "                                          cv=cv, scoring=cost)\n",
    "\n",
    "    # Truncate parameter values to display them as ticks on the x-axis.\n",
    "    x_axis = ['{0:.4f}'.format(p) for p in param_values]\n",
    "\n",
    "    # Plot the validation curve.\n",
    "    plt.plot(x_axis, train_err, color='#ba4562', label='Training Error')\n",
    "    plt.plot(x_axis, val_err, color='#3e8674', label='Validation Error')\n",
    "    plt.title('Validation Curve')\n",
    "    plt.xticks(rotation=75)\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Return hyperparameter value that minimizes validation error.\n",
    "    return param_values[val_err.argmin()]\n",
    "\n",
    "\n",
    "# Plot a learning curve using the given estimator and cost function.\n",
    "def plot_learning_curve(estimator, cost):\n",
    "    # Compute the errors of the training and validation\n",
    "    # data across several different training sizes.\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_err, val_err = learning_curve(estimator=estimator,\n",
    "                                                     X=X, y=y,\n",
    "                                                     train_sizes=train_sizes,\n",
    "                                                     cv=cv, scoring=cost)\n",
    "\n",
    "    # Plot the learning curve.\n",
    "    plt.plot(train_sizes, train_err, color='#b45f06', label='Training Error')\n",
    "    plt.plot(train_sizes, val_err, color='#6aa84f', label='Validation Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the ROC curve(s) using the given estimator(s).\n",
    "def plot_roc_curve(*estimators):\n",
    "    for estimator in estimators:\n",
    "        # Compute the ROC and AUC using the prediction probabilities.\n",
    "        y_proba = estimator.predict_proba(X_test)[:, 1]\n",
    "        FPR, TPR, thresholds = roc_curve(y_test, y_proba)\n",
    "        AUC = roc_auc_score(y_test, y_proba)\n",
    "        model_name = get_model_name(estimator)\n",
    "\n",
    "        # Plot the ROC curve. The AUC is listed in the legend.\n",
    "        plt.plot(FPR, TPR, label=f'{model_name} (AUC = {AUC:.4})')\n",
    "\n",
    "    # Plot the TPR=FPR line, title, axis labels, and legend.\n",
    "    plt.plot([0, 1], [0, 1], color='darkred', linestyle='--')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim([0.0, 1.001])\n",
    "    plt.ylim([0.0, 1.001])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot a grid search by varying the given parameters as specified.\n",
    "def plot_grid_search(estimator, param_grid, cv=cv):\n",
    "    # Perform the grid search.\n",
    "    grid = GridSearchCV(estimator, param_grid, cv=cv).fit(X, y)\n",
    "    scores = grid.cv_results_['mean_test_score']\n",
    "    param_1, param_2 = tuple(param_grid)\n",
    "    param_2_values = list(map(str, param_grid[param_2]))\n",
    "    num_values = len(param_2_values)\n",
    "\n",
    "    for idx, val in enumerate(param_grid[param_1]):\n",
    "        # Get the grid search scores from varying the second\n",
    "        # parameter and keeping the first parameter the same.\n",
    "        idx *= num_values\n",
    "        param_1_scores = scores[idx:idx + num_values]\n",
    "        \n",
    "        # Create a separate label for each value of the first parameter.\n",
    "        label = f'{get_param_name(param_1)} = {val}'\n",
    "        plt.plot(param_2_values, param_1_scores, marker='o', label=label)\n",
    "\n",
    "    # Plot the second parameter on the x-axis.\n",
    "    plt.title('Grid Search')\n",
    "    plt.xlabel(get_param_name(param_2))\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Return hyperparameter values that maximize the grid search score.\n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the main classification metrics.\n",
    "def print_classification_report(estimator):\n",
    "    # Generate confusion matrix from predictions.\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "\n",
    "    # Compute accuracy, precision, recall, and F1 score.\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # Print the unpacked confusion matrix and computed metrics.\n",
    "    print(f'TP = {TP}, TN = {TN}, FP = {FP}, FN = {FN}')\n",
    "    print(f'Accuracy = {accuracy}')\n",
    "    print(f'Precision = {precision}')\n",
    "    print(f'Recall = {recall}')\n",
    "    print(f'F1 Score = {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Logistic Regression</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain feature scaling and regularized logistic regression.\n",
    "log_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('Logistic Regression', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Log loss will be used as the cost function here.\n",
    "log_cost = make_scorer(log_loss, needs_proba=True)\n",
    "\n",
    "# The regularization strength, C ~ 1/λ, will be varied here.\n",
    "reg = 'Logistic Regression__C'\n",
    "reg_values = np.geomspace(0.001, 1, 25)\n",
    "\n",
    "# Get best value of C from the validation curve.\n",
    "C = plot_validation_curve(log_clf, reg, reg_values, log_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply logistic regression using the tuned hyperparameter.\n",
    "log_clf.set_params(**{reg: C})\n",
    "\n",
    "# Plot the learning curve.\n",
    "plot_learning_curve(log_clf, log_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data.\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the ROC curve and AUC score.\n",
    "plot_roc_curve(log_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification metrics.\n",
    "print_classification_report(log_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Support Vector Machine</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain feature scaling and support vector classification.\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('SVM', SVC())\n",
    "])\n",
    "\n",
    "# Hinge loss will be used as the cost function here.\n",
    "svm_cost = make_scorer(hinge_loss)\n",
    "\n",
    "# Use a grid search to tune multiple hyperparameters.\n",
    "param_grid = {\n",
    "    'SVM__C': [0.1, 1, 10, 100, 1000],\n",
    "    'SVM__gamma': [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "}\n",
    "params = plot_grid_search(svm_clf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test grid search results for reproducibility.\n",
    "def plot_grid_searches(seed_range):\n",
    "    # Insert initial grid search results.\n",
    "    grid_results = pd.DataFrame(columns=list(map(get_param_name, params)))\n",
    "    grid_results.loc[42] = params.values()\n",
    "\n",
    "    # Perform a grid search for each seed in the given range.\n",
    "    for i in seed_range:\n",
    "        X_train, X_val, *_ = train_val_test_split(X, y, seed=i)\n",
    "        seed_cv = [(X_train.index, X_val.index)]\n",
    "        param_results = plot_grid_search(svm_clf, param_grid, seed_cv)\n",
    "        grid_results.loc[i] = param_results.values()\n",
    "\n",
    "    # Display parameter results in a table.\n",
    "    display(grid_results)\n",
    "\n",
    "\n",
    "# plot_grid_searches(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply SVM using the tuned hyperparameters.\n",
    "svm_clf.set_params(**params)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plot_learning_curve(svm_clf, svm_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable probability estimates (required for ROC curve).\n",
    "svm_clf.set_params(**{'SVM__probability': True})\n",
    "\n",
    "# Fit the model to the training data.\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the ROC curve and AUC score.\n",
    "plot_roc_curve(svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification metrics.\n",
    "print_classification_report(svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Logistic Regression with<br>Polynomial Features</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain polynomial features, feature scaling, and logistic regression.\n",
    "poly_clf = Pipeline([\n",
    "    ('polyfeatures', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('Polynomial LR', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# The regularization strength, C ~ 1/λ, will be varied here.\n",
    "reg = 'Polynomial LR__C'\n",
    "reg_values = np.geomspace(0.001, 1, 25)\n",
    "\n",
    "# Get the best value of C from the validation curve.\n",
    "C = plot_validation_curve(poly_clf, reg, reg_values, log_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply logistic regression using the tuned hyperparameter.\n",
    "poly_clf.set_params(**{reg: C})\n",
    "\n",
    "# Plot the learning curve.\n",
    "plot_learning_curve(poly_clf, log_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data.\n",
    "poly_clf.fit(X_train, y_train)\n",
    "\n",
    "# Plot the ROC curve and AUC score.\n",
    "plot_roc_curve(poly_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification metrics.\n",
    "print_classification_report(poly_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Analyze Success</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can compare the success of each model used.\n",
    "models = [log_clf, svm_clf, poly_clf]\n",
    "\n",
    "# Plot the ROC curves and AUC scores of each model.\n",
    "plot_roc_curve(*models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors each model makes in a three-way Venn diagram.\n",
    "misclassified_sets = tuple(map(misclassified_labels, models))\n",
    "model_names = tuple(map(get_model_name, models))\n",
    "venn3_unweighted(misclassified_sets, model_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample false positives from the given set of error indices.\n",
    "def show_false_positives(error_idx):\n",
    "    df = data_test.loc[list(error_idx)].query('VSCode == 0')\n",
    "    print(f'False Positives = {len(df)}')\n",
    "    display(df.sample(5, random_state=42) if len(df) >= 5 else df)\n",
    "\n",
    "\n",
    "# Sample false negatives from the given set of error indices.\n",
    "def show_false_negatives(error_idx):\n",
    "    df = data_test.loc[list(error_idx)].query('VSCode == 1')\n",
    "    print(f'False Negatives = {len(df)}')\n",
    "    display(df.sample(5, random_state=42) if len(df) >= 5 else df)\n",
    "\n",
    "\n",
    "# Combine the design matrix and output vector for the test data.\n",
    "data_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
    "shared_errors = set.intersection(*misclassified_sets)\n",
    "all_errors = set.union(*misclassified_sets)\n",
    "log_errors, svm_errors, poly_errors = misclassified_sets\n",
    "\n",
    "# Display samples from the shared errors across learning methods.\n",
    "print('Shared Errors:')\n",
    "show_false_negatives(shared_errors)\n",
    "show_false_positives(shared_errors)\n",
    "\n",
    "# Display samples from the unique errors of each learning method.\n",
    "for model_name, clf_errors in zip(model_names, misclassified_sets):\n",
    "    print(f'{model_name} Errors:')\n",
    "    unique_errors = all_errors.copy()\n",
    "\n",
    "    for misclassified in misclassified_sets:\n",
    "        if clf_errors is not misclassified:\n",
    "            unique_errors -= misclassified\n",
    "\n",
    "    show_false_negatives(unique_errors)\n",
    "    show_false_positives(unique_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb9046b089dd11987e26c966e9cd61a5c9ff6995a79404100dca27b4da6e48fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
